{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from batchbald_redux import repeated_mnist\n",
    "from main.models import ConvNet\n",
    "from main.training_models import train_model\n",
    "from main.bald_sampling import compute_entropy, compute_conditional_entropy\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "train_dataset, val_dataset = repeated_mnist.create_MNIST_dataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
    "                                           sampler=torch.utils.data.SubsetRandomSampler(range(80)))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(range(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.223040819168091\n",
      "Epoch 2/20, Loss: 1.9746544361114502\n",
      "Epoch 3/20, Loss: 1.6898173093795776\n",
      "Epoch 4/20, Loss: 1.3834806680679321\n",
      "Epoch 5/20, Loss: 0.9185072779655457\n",
      "Epoch 6/20, Loss: 0.694076657295227\n",
      "Epoch 7/20, Loss: 0.503821611404419\n",
      "Epoch 8/20, Loss: 0.13865363597869873\n",
      "Epoch 9/20, Loss: 0.251486599445343\n",
      "Epoch 10/20, Loss: 0.14383897185325623\n",
      "Epoch 11/20, Loss: 0.033987682312726974\n",
      "Epoch 12/20, Loss: 0.08825220167636871\n",
      "Epoch 13/20, Loss: 0.02064281888306141\n",
      "Epoch 14/20, Loss: 0.014900978654623032\n",
      "Epoch 15/20, Loss: 0.020352903753519058\n",
      "Epoch 16/20, Loss: 0.005083861760795116\n",
      "Epoch 17/20, Loss: 0.0029607515316456556\n",
      "Epoch 18/20, Loss: 0.0059080119244754314\n",
      "Epoch 19/20, Loss: 0.0046522533521056175\n",
      "Epoch 20/20, Loss: 0.0045122671872377396\n",
      "Accuracy: 0.65625\n"
     ]
    }
   ],
   "source": [
    "# train model and optimize\n",
    "model = ConvNet()\n",
    "\n",
    "# train model\n",
    "model = train_model(model, train_loader, num_epochs=20)\n",
    "\n",
    "# compute out of sample performance (crudely)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "    print('Accuracy:', torch.mean((y_pred.argmax(dim=1) == y_test).float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prior precision is tensor([11.0047]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:409: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# linearize and laplace approximation\n",
    "la = laplace.Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='kron', temperature=1e-3)\n",
    "la.fit(train_loader)\n",
    "la.optimize_prior_precision(pred_type='glm', method='marglik', link_approx='probit', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BALD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing entropy (first term in BALD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.bald_sampling import compute_entropy, compute_entropy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = compute_entropy(la_model=la, data=x_test)\n",
    "ent_w = compute_entropy_weights(la_model=la, data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6447, 1.5470, 1.7109, 1.3268, 1.3466, 1.6111, 1.5681, 1.6632, 1.3571,\n",
       "        1.6855, 1.4234, 1.7456, 0.7215, 1.9799, 1.7025, 1.6693, 1.3543, 1.6446,\n",
       "        1.7003, 1.4787, 1.9724, 1.7405, 1.8122, 1.4522, 1.3558, 0.7002, 1.4526,\n",
       "        1.5812, 1.3970, 1.5918, 1.6652, 1.2756])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0600, 2.1252, 2.0115, 2.1458, 2.0465, 2.1216, 2.0717, 2.0842, 2.0722,\n",
       "        2.1786, 2.0694, 2.1631, 1.9006, 2.1552, 2.0533, 1.9263, 2.1923, 2.0608,\n",
       "        1.9933, 2.1623, 2.0291, 2.0901, 2.1564, 2.0800, 1.8979, 1.8932, 2.0041,\n",
       "        2.1570, 2.1095, 2.0825, 2.1016, 2.0035])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign weights to a parameter\n",
    "def set_last_linear_layer_combined(model, new_weights_and_bias):\n",
    "    # Find the last linear layer\n",
    "    last_linear_layer = None\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            last_linear_layer = module\n",
    "    \n",
    "    if last_linear_layer is None:\n",
    "        raise ValueError(\"No linear layer found in the model\")\n",
    "\n",
    "    # Get the shapes\n",
    "    out_features, in_features = last_linear_layer.weight.shape\n",
    "    \n",
    "    # Check if the input tensor has the correct shape\n",
    "    expected_shape = (out_features * in_features + out_features,)\n",
    "    if new_weights_and_bias.shape != expected_shape:\n",
    "        raise ValueError(f\"Input tensor shape {new_weights_and_bias.shape} doesn't match the expected shape {expected_shape}\")\n",
    "\n",
    "    # Split the input tensor into weights and bias\n",
    "    new_weights = new_weights_and_bias[:out_features * in_features].reshape(out_features, in_features)\n",
    "    new_bias = new_weights_and_bias[out_features * in_features:]\n",
    "\n",
    "    # Set new weights and bias\n",
    "    last_linear_layer.weight.data = new_weights\n",
    "    last_linear_layer.bias.data = new_bias\n",
    "\n",
    "    return last_linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prior precision is tensor([36.8790]).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "First we sample from the posterior of p(weights | data)\n",
    "\n",
    "For each weight sample, we compute the predictive distribution p(y | x, weights),\n",
    "by passing x through the model with the sampled weights and applying the softmax function.\n",
    "\n",
    "Doing this for many samples, we can compute the entropy of the predictive distribution at each x.\n",
    "\n",
    "'''\n",
    "# linearize and laplace approximation\n",
    "la = laplace.Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='kron', temperature=1e-3)\n",
    "la.fit(train_loader)\n",
    "la.optimize_prior_precision(pred_type='glm', method='marglik', link_approx='probit', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prior precision is tensor([16.4651]).\n",
      "Optimized prior precision is tensor([17.4157]).\n",
      "Optimized prior precision is tensor([16.7425]).\n",
      "Optimized prior precision is tensor([17.2752]).\n",
      "Optimized prior precision is tensor([16.7267]).\n",
      "Optimized prior precision is tensor([15.8028]).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[279], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m set_last_linear_layer_combined(la\u001b[38;5;241m.\u001b[39mmodel, weights)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Optimise the prior precision\u001b[39;00m\n\u001b[0;32m     14\u001b[0m la\u001b[38;5;241m.\u001b[39moptimize_prior_precision(pred_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglm\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarglik\u001b[39m\u001b[38;5;124m'\u001b[39m, link_approx\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobit\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\lllaplace.py:194\u001b[0m, in \u001b[0;36mLLLaplace.fit\u001b[1;34m(self, train_loader, override, progress_bar)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_mean: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_mean\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_H()\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m parameters_to_vector(\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_backprop:\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:1509\u001b[0m, in \u001b[0;36mKronLaplace.fit\u001b[1;34m(self, train_loader, override, progress_bar)\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;66;03m# discount previous Kronecker factors to sum up properly together with new ones\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_facs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rescale_factors(\n\u001b[0;32m   1506\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_facs, n_data_old \u001b[38;5;241m/\u001b[39m (n_data_old \u001b[38;5;241m+\u001b[39m n_data_new)\n\u001b[0;32m   1507\u001b[0m     )\n\u001b[1;32m-> 1509\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_facs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_facs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:696\u001b[0m, in \u001b[0;36mParametricLaplace.fit\u001b[1;34m(self, train_loader, override, progress_bar)\u001b[0m\n\u001b[0;32m    693\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress_bar)\n\u001b[0;32m    694\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Computing Hessian]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMutableMapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# To support Huggingface dataset\u001b[39;49;00m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict_key_y\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\tqdm\\std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[1;32m-> 1169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample from the posterior\n",
    "posterior_weights = la.sample(n_samples=50)\n",
    "entropies = torch.zeros(posterior_weights.shape[0], x_test.shape[0])\n",
    "\n",
    "# Compute the entropy for each sample\n",
    "for i, weights in enumerate(posterior_weights):\n",
    "    # Set the weights in the model\n",
    "    set_last_linear_layer_combined(la.model, weights)\n",
    "\n",
    "    # fit the model\n",
    "    la.fit(train_loader)\n",
    "\n",
    "    # Optimise the prior precision\n",
    "    la.optimize_prior_precision(pred_type='glm', method='marglik', link_approx='probit', verbose=True)\n",
    "\n",
    "    # Compute the predictive distribution\n",
    "    probs = la(x_test, pred_type='glm', link_approx='probit')\n",
    "\n",
    "    # Compute the entropy\n",
    "    entropies[i] = _h(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALD has 0 zeros out of 32 samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4202, 1.6857, 0.9458, 1.6858, 1.4193, 1.3705, 1.0489, 1.5071, 0.8167,\n",
       "        1.5586, 1.3837, 1.3683, 1.4766, 1.7605, 1.7392, 0.9636, 1.2336, 0.7664,\n",
       "        0.8247, 0.9784, 1.7967, 1.4989, 1.0402, 0.9998, 1.7251, 1.8253, 1.4178,\n",
       "        1.7722, 1.1391, 0.9367, 1.2260, 1.3115])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average over sampled weights\n",
    "entropies_avg = entropies.mean(dim=0)\n",
    "\n",
    "print(f'BALD has {torch.sum(ent < entropies_avg).item()} zeros out of {ent.shape[0]} samples.')\n",
    "bald = torch.max(ent - entropies_avg, torch.zeros_like(ent))\n",
    "bald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prior precision is tensor([29.4391]).\n",
      "Optimized prior precision is tensor([14.3348]).\n",
      "Optimized prior precision is tensor([14.2572]).\n",
      "Optimized prior precision is tensor([14.9633]).\n",
      "Optimized prior precision is tensor([14.3452]).\n",
      "Optimized prior precision is tensor([15.4623]).\n",
      "Optimized prior precision is tensor([15.0644]).\n",
      "Optimized prior precision is tensor([14.3005]).\n",
      "Optimized prior precision is tensor([14.8923]).\n",
      "Optimized prior precision is tensor([14.0761]).\n",
      "Optimized prior precision is tensor([14.5439]).\n",
      "Optimized prior precision is tensor([14.1053]).\n",
      "Optimized prior precision is tensor([15.2824]).\n",
      "Optimized prior precision is tensor([15.5675]).\n",
      "Optimized prior precision is tensor([13.0978]).\n",
      "Optimized prior precision is tensor([12.8305]).\n",
      "Optimized prior precision is tensor([14.6344]).\n",
      "Optimized prior precision is tensor([14.2011]).\n",
      "Optimized prior precision is tensor([14.5757]).\n",
      "Optimized prior precision is tensor([14.7716]).\n",
      "Optimized prior precision is tensor([15.0757]).\n",
      "Optimized prior precision is tensor([14.4679]).\n",
      "Optimized prior precision is tensor([13.6938]).\n",
      "Optimized prior precision is tensor([15.1361]).\n",
      "Optimized prior precision is tensor([14.2088]).\n",
      "Optimized prior precision is tensor([13.0615]).\n",
      "Optimized prior precision is tensor([14.1261]).\n",
      "Optimized prior precision is tensor([13.2823]).\n",
      "Optimized prior precision is tensor([13.9313]).\n",
      "Optimized prior precision is tensor([14.8835]).\n",
      "Optimized prior precision is tensor([15.3262]).\n",
      "Optimized prior precision is tensor([13.9920]).\n",
      "Optimized prior precision is tensor([13.9478]).\n",
      "Optimized prior precision is tensor([13.9348]).\n",
      "Optimized prior precision is tensor([15.4594]).\n",
      "Optimized prior precision is tensor([15.4286]).\n",
      "Optimized prior precision is tensor([11.9332]).\n",
      "Optimized prior precision is tensor([14.9049]).\n",
      "Optimized prior precision is tensor([14.6855]).\n",
      "Optimized prior precision is tensor([16.6393]).\n",
      "Optimized prior precision is tensor([13.8814]).\n",
      "Optimized prior precision is tensor([14.8556]).\n",
      "Optimized prior precision is tensor([13.2706]).\n",
      "Optimized prior precision is tensor([14.6833]).\n",
      "Optimized prior precision is tensor([13.6344]).\n",
      "Optimized prior precision is tensor([15.6421]).\n",
      "Optimized prior precision is tensor([13.1537]).\n",
      "Optimized prior precision is tensor([14.8935]).\n",
      "Optimized prior precision is tensor([14.6576]).\n",
      "Optimized prior precision is tensor([14.5834]).\n",
      "Optimized prior precision is tensor([13.3008]).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4505, 1.7123, 0.9928, 1.6769, 1.3852, 1.3303, 1.0606, 1.4980, 0.7725,\n",
       "        1.5536, 1.3738, 1.3918, 1.4747, 1.7254, 1.7494, 0.9781, 1.1987, 0.7730,\n",
       "        0.8604, 1.0272, 1.7938, 1.4872, 1.1301, 1.0425, 1.7671, 1.8071, 1.4647,\n",
       "        1.7551, 1.2098, 0.9509, 1.2345, 1.3743])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main.bald_sampling import compute_bald\n",
    "# linearize and laplace approximation\n",
    "la = laplace.Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='kron', temperature=1e-3)\n",
    "la.fit(train_loader)\n",
    "la.optimize_prior_precision(pred_type='glm', method='marglik', link_approx='probit', verbose=True)\n",
    "\n",
    "bald = compute_bald(la, x_test, train_loader, n_samples=50)\n",
    "bald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale of values is looking pretty plausible when compared to the values obtained from bald as in batchbald_redux.\n",
    "This is using sampling from the parameter space. Now try to do sampling in f-space to compute *conditional entropy*, analagous to how Houlsby describes it. If that yields the same values it could be much quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(la_model, data, train_loader, refit=True, n_samples=50):\n",
    "    # Sample from the posterior\n",
    "    posterior_weights = la_model.sample(n_samples=n_samples)\n",
    "    entropies = torch.zeros(posterior_weights.shape[0], data.shape[0])\n",
    "\n",
    "    # Compute the entropy for each sample\n",
    "    for i, weights in enumerate(posterior_weights):\n",
    "        # Set the weights in the model\n",
    "        set_last_linear_layer_combined(la_model.model, weights)\n",
    "\n",
    "        if refit:\n",
    "            # fit the model\n",
    "            la_model.fit(train_loader)\n",
    "\n",
    "            # Optimise the prior precision\n",
    "            la_model.optimize_prior_precision(pred_type='glm', method='marglik', link_approx='probit', verbose=False)\n",
    "\n",
    "        # Compute the predictive distribution\n",
    "        probs = la_model(data, pred_type='glm', link_approx='probit')\n",
    "\n",
    "        # Compute the entropy\n",
    "        entropies[i] = _h(probs)\n",
    "\n",
    "    return entropies.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2419, 2.2292, 2.2521, 2.2292, 2.2303, 2.2296, 2.2470, 2.2322, 2.2734,\n",
       "        2.2292, 2.2525, 2.2308, 2.2298, 2.2304, 2.2292, 2.2655, 2.2714, 2.2305,\n",
       "        2.2593, 2.2741, 2.2292, 2.2292, 2.2609, 2.2346, 2.2292, 2.2292, 2.2292,\n",
       "        2.2294, 2.2766, 2.2786, 2.2330, 2.2293])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = compute_entropy(la, x_test)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\Statistics\\TT\\msc_thesis\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:409: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.2745, 2.2632, 2.2592, 2.2210, 2.2492, 2.2516, 2.2389, 2.2652, 2.2517,\n",
       "        2.2552, 2.2774, 2.2649, 2.2410, 2.2399, 2.2166, 2.2865, 2.2378, 2.2657,\n",
       "        2.2585, 2.2596, 2.2477, 2.2480, 2.2557, 2.2832, 2.2004, 2.2211, 2.2495,\n",
       "        2.2393, 2.2542, 2.2793, 2.2512, 2.2730])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce = compute_conditional_entropy(la, x_test, train_loader, refit=True, n_samples=10)\n",
    "ce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
